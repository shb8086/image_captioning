\relax 
\citation{Karpathy2015,Vinyals2015,Xu2015,Luo2023}
\citation{MSCOCO}
\citation{Flickr8k}
\citation{Flickr30k}
\@writefile{toc}{\contentsline {title}{Expanding Flicker30k: a Novel Dataset for Persian Language Captions}{1}{}\protected@file@percent }
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{Shima Baniadamdizaj \and Alexander Breuer }{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\citation{Multi30k}
\citation{Xue,Zoph,Rosa}
\citation{Flickr30k}
\citation{MSCOCO}
\citation{Flickr8k}
\citation{Flickr30k}
\citation{Multi30k}
\citation{VIST}
\citation{MSCOCO}
\citation{Flickr8k}
\@writefile{toc}{\contentsline {section}{\numberline {2}Motivation and Related Works}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}English Captioned Datasets}{2}{}\protected@file@percent }
\citation{Flickr30k}
\citation{Nocaps}
\citation{Openimages}
\citation{VizWiz}
\citation{Multi30k}
\citation{Flickr30k}
\citation{Korean}
\citation{ImageCLEF2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Non-English Image Captioning Datasets}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Persian Image Captioning Datasets}{4}{}\protected@file@percent }
\citation{Flickr30k}
\citation{Flickr30k}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dataset Description}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Image Collection}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Annotation Procedure}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Content Analysis}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The example images from dataset with (a)longest and (b)shortest Persian captions.}}{7}{}\protected@file@percent }
\newlabel{fig2}{{1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distribution of word lengths in captions and number of captions per word length.}}{8}{}\protected@file@percent }
\newlabel{fig3}{{2}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments, Results, and Discussions}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Dataset Preprocessing}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Image Feature Extractor}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Transformer-decoder architecture}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Model Specifications}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Results}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and Validation Performance of the Model. The plot displays the changes in accuracy (a) and loss(b) during the training (blue) and validation (orange) process.}}{11}{}\protected@file@percent }
\newlabel{fig4}{{3}{11}}
\bibcite{Karpathy2015}{1}
\bibcite{Vinyals2015}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Predicted caption and attention maps for captioning an image with an elephant walking.}}{12}{}\protected@file@percent }
\newlabel{fig5}{{4}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Future Work}{12}{}\protected@file@percent }
\bibcite{Xu2015}{3}
\bibcite{Luo2023}{4}
\bibcite{MSCOCO}{5}
\bibcite{Flickr8k}{6}
\bibcite{Flickr30k}{7}
\bibcite{Multi30k}{8}
\bibcite{Xue}{9}
\bibcite{Zoph}{10}
\bibcite{Rosa}{11}
\bibcite{VIST}{12}
\bibcite{Nocaps}{13}
\bibcite{Openimages}{14}
\bibcite{VizWiz}{15}
\bibcite{Korean}{16}
\bibcite{ImageCLEF2018}{17}
\gdef \@abspage@last{13}
